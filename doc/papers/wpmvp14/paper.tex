% submit to https://sites.google.com/site/wpmvp2014/home
\documentclass[preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}


\begin{document}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

%\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country} 
%\copyrightyear{20yy} 
%\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
%\doi{nnnnnnn.nnnnnnn}

\title{Vectorizing Python Constructs Using Pythran and Boost SIMD}

\authorinfo{Serge Guelton}
           {QuarksLab, T{\'e}l{\'e}com Bretagne}
           {sguelton@quarkslab.com}
\authorinfo{Jo{\"e}l Falcou}
           {MetaScale}
           {joel.falcou@metascale.org}

\maketitle

\begin{abstract}

    The Python language is highly dynamic, most notably due to late binding. As
    a consequence, program run using Python typically run an order of magnitude
    slower than their C counterpart. It is also a high level languages whose
    semantic can be made more static without much change from a user point of
    view in the case of mathematical applications. In that case, the language
    provides several vectorization opportunities that are studied in this
    paper, and evaluated in the context of Pythran, an ahead-of-time compiler
    that turns Python module into C++ meta-programs.

\end{abstract}

%\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore, 
% you may leave them out
%\terms
%term1, term2

\keywords
Vectorization, Meta-Programming, Python, C++


%%
%%
\section{Python, Unboxing and Pythran}

The Python language has grown in audience for the past ten years, even reaching
the world of scientific computations thanks to the \texttt{numpy} module, a
module that provides a MATLAB-like API. As a consequence, more and more code is
being written either in pure Python, generally to prototype an application, or
as a Python and native code mix when performance matters. However, Python
trades performance for dynamicity and does not particularly shines in terms of
performance.

For instance, the natural solution to Project Euler sixth problem ``Find the
difference between the sum of the squares of the first one hundred natural
numbers and the square of the sum`` can be coded as in
Listing~\ref{lst:euler06-py} in Python and as in Listing~\ref{lst:euler06-c} in
C. A comparison of the performance of the Python function and the C function
called through the \texttt{ctypes} module shows that the C version runs more
than $\times250$ faster than the Python version. Enabling compiler
auto-vectorization makes the C version $\times400$ faster than the Python
version.~\footnote{To perform more reliable time measurements, \texttt{n} has
    been increased to 100001 instead of 101. The script used to perform the
    measures as well as all other experimental data are available at
\url{https://github.com/serge-sans-paille/pythran/tree/wpmvp14/doc/papers/wpmvp14/experiments}.}

There are two main reason for the poor performance of Python:

\begin{description}

    \item[dynamic binding] each variable access is made through a dictionnary
        look-up to bind variable names to variable instances~;

    \item[boxing] each variable value is encapsulated into a heap-allocated
        generic object ---the \texttt{PyObject}--- which implies extra
        indirections for each operation.

\end{description}

In that context, it does not make sense to speak about vectorization: the
nature of an operation is unknown to the interpreter until its execution, and
data are not contiguous in memory.

\lstinputlisting[language=python, label={lst:euler06-py}, caption={Solution to the Project Euler Sixth Problem in Python}]{experiments/euler06.py}

\lstinputlisting[language=c, label={lst:euler06-c}, caption={Solution to the Project Euler Sixth Problem in C}]{experiments/euler06.c}

Several approaches have been proposed to solve these issues: using native
modules, relying on Ahead-Of-Time (AOT) or Just-In-Time (JIT) compilation.

A native module is a shared library whose interface make it callable from the
Python interpreter using the regular \texttt{import} mechanism. It makes it
possible to call FORTRAN/C/C++ code from Python in a transparent way. Typically
Python object are unboxed when transferred from Python to C, where efficient
computations can happen. The the result is boxed when getting back to the
Python world. In between, vectorization can happen.  This is the approach taken
by the \texttt{numpy} module: a native type, the \texttt{ndarray} contains
contiguous unboxed elemnts, and provides a great deal of high-level functions
to manipulate them. All the computation-intensive part is done in pure C.
Vectorization can eventually happen at this level, for instance when performing
a point-to-point operation like the sum of two arrays.  However the granuarity
of the operations limits the ratio of load / stores versus operation. For
instance, if \texttt{a}, \texttt{b} and \texttt{c} are 1-D arrays, the sum
\texttt{a + b +c} is computed using two loops and an intermediate array.

AOT compilation uses type inference or type annotation to statically compute
the type of all (like in Shedskin, Parakeet) or part of (like in Cython or
Numba) variables, effectively unboxing them to speed-up computations. Using JIT
compilation, type information is available at runtime and the unboxing decision
is based on profitablity, e.g.\ according to a trace analysis. This is the
approach of the PyPy project. In both cases, generation of vector instruction
is possible, but none of the cited project use it.  Indeed automatic
vectorization is a complex topic, and as stated in~\cite{maleki2011}, many cases
are not yet correctly supported even for mainstream compilers.

Pythran is also an AOT compiler, which holds the uinique property of turning
Python modules into C++ \emph{meta-programs}. It does not need type annotations
and keeps the polymorphism of original functions, respecting Python's duck
typing. Like other AOT compilers, it unboxes all variables for better
performance, and performs a wide variety of optimizations such as lazy
evaluation, constant folding, expression fusion etc.  It supports core
\texttt{numpy} constructs and most Python constructs to the notable exception
of user classes. Constructs such as \texttt{eval}, are not supported.

This paper introduces vectorization of Python programs based on the idea that
many Python constructs already exhibit good vectorization opportunities. The
goal of the paper switches from automatic vectorization to vectorization of
high-level Python intrisics, as described in Section~\ref{sec:python-semantic}.
One of the difficulties of vectorizing Python functions is linked to function
polymorphism. Vectorization in the context of C++ meta-programs is examined in
Section~\ref{sec:meta-vectorization}. The approach is validated using the
Pythran compiler on several micro-applications from signal processing, physics
and mathematics presented in Section~\ref{sec:benchs}.

%%
%%
\section{Using Python Semantic for Vectorization}
\label{sec:python-semantic}

\subsection{Intrinsics}

\subsection{List Comprehension}

\subsection{The Numpy Module}

%%
%%
\section{Vectorization of a Meta-Program}
\label{sec:meta-vectorization}

\subsection{Boost SIMD}

\subsection{Load/Store Protocol}

%%
%%
\section{Validation of the Approach using the Pythran Compiler}
\label{sec:benchs}

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.
\cite{*}

\bibliographystyle{abbrvnat}
\bibliography{biblio}


\end{document}
